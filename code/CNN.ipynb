{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6fab769-7359-4a8a-bd3f-0f2ff464231c",
   "metadata": {},
   "source": [
    "# EfficientNet CNN\n",
    "\n",
    "Notebook by Martijn de Vries <br>\n",
    "martijndevries91@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbb251d-5b88-4f2a-ab17-c62a89ce4559",
   "metadata": {},
   "source": [
    "NOTE: the EfficientNetB0 model that I use in this notebook has trouble saving with some version of tensorflow (probably 2.10 and higher) <br>\n",
    "I used the following fix to make it work on my machine (found in the comments here: https://github.com/keras-team/keras/issues/17199):\n",
    "\n",
    "location: lib/python3.10/site-packages/keras/applications/efficientnet.py (py3.10) <br>\n",
    "EDIT this: <br>\n",
    "<code> x = layers.Rescaling(1.0 / tf.math.sqrt(IMAGENET_STDDEV_RGB))(x) </code> <br>\n",
    "TO: <br>\n",
    "<code> x = layers.Rescaling(\n",
    "    [1.0 / math.sqrt(stddev) for stddev in IMAGENET_STDDEV_RGB]\n",
    ")(x) </code>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e447d2cd-c9aa-4237-a1b8-bafff7ed7028",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-03 10:57:30.511871: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "#tensorflow\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img, smart_resize\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from tensorflow.keras.applications import EfficientNetB0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d14b2c3-3868-4164-be57-2dfc885520d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a360b6-1216-4c74-9741-53817ce1c27a",
   "metadata": {},
   "source": [
    "I can use image_dataset_from_directory to load in the images.\n",
    "Note that even though the images are grayscale, i have to use rgb color mode because that's the format EfficientNet requires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e41751b1-f19a-4bd4-9088-50d646458e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 93020 files belonging to 100 classes.\n",
      "Using 79067 files for training.\n",
      "Found 93020 files belonging to 100 classes.\n",
      "Using 13953 files for validation.\n"
     ]
    }
   ],
   "source": [
    "train_ds = image_dataset_from_directory('../img_data/train_symbols/', image_size=(100,100),\n",
    "                                      batch_size=200, seed=123, validation_split=0.15,\n",
    "                                       subset='training', labels ='inferred', color_mode='rgb', label_mode='categorical')  \n",
    "\n",
    "val_ds = image_dataset_from_directory('../img_data/train_symbols/', image_size=(100,100),\n",
    "                                      batch_size=200, seed=123, validation_split=0.15,\n",
    "                                       subset='validation', labels ='inferred', color_mode='rgb', label_mode='categorical')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3353f501-2a7d-4626-8cbf-a855dc172db9",
   "metadata": {},
   "source": [
    "Let's look at the class names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03739932-5475-47d4-92f0-e3a5278ffc66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label_!', 'label_(', 'label_)', 'label_+', 'label_,']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.class_names[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab661af-f3d1-4f0b-9eb0-3b5882da2286",
   "metadata": {},
   "source": [
    "I'll want to save these, so I can access them when it's time to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e538f926-c23a-48c9-94b7-fc7238044ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../class_names.txt', 'w') as f:\n",
    "    for i, label in enumerate(train_ds.class_names):\n",
    "        f.write(str(i)+  ' ' + label + '\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bee089-042f-4d32-983e-13fe80355178",
   "metadata": {},
   "source": [
    "Let's also look at the images that are loaded in by image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18858742-b54d-4b01-87e7-8c0f9646eb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = []\n",
    "for images, labels in train_ds.take(1):\n",
    "    for i in range(30):\n",
    "        img_list.append(images[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ece2f1a-5b60-4633-8e91-39f2ec964b91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([100, 100, 3])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_list[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "49053b1c-4992-46f3-9809-294971a4c149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img_list[11][:,:,0], cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801dcced-9ca1-4e44-a0da-8215cbc4e9a0",
   "metadata": {},
   "source": [
    "That looks alright! It's important that the size of the symbols is roughtly similar to the symbol size out of the pre-processing pipeline.\n",
    "\n",
    "Now we can set up the neural network. For optimal performance, I'll use the EfficientNetB0 network, and re-train it using my data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8c966a2-2f01-4348-ba55-eee6f746d504",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df7b8990-e181-425f-9e7a-316f186be278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " efficientnetb0 (Functional)  (None, 1280)             4049571   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1280)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                40992     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               3300      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,093,863\n",
      "Trainable params: 4,051,840\n",
      "Non-trainable params: 42,023\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "efficient_net = EfficientNetB0(\n",
    "    weights='imagenet',\n",
    "    input_shape=(100, 100, 3),\n",
    "    include_top=False,\n",
    "    pooling='max'\n",
    ")\n",
    "\n",
    "model = Sequential()\n",
    "model.add(efficient_net)\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(units = 32, activation='relu'))\n",
    "model.add(Dense(units = 100, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate = 0.0005),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df9ff22-c268-41bb-b185-7982472d0b3f",
   "metadata": {},
   "source": [
    "Now we can fit the model. NB: on my local machine this took about 4hrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc6d7cb9-feb3-4ce1-a8e1-b24c41dfcbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "396/396 [==============================] - 1311s 3s/step - loss: 0.8973 - accuracy: 0.8047 - val_loss: 0.3688 - val_accuracy: 0.9106\n",
      "Epoch 2/30\n",
      "396/396 [==============================] - 1269s 3s/step - loss: 0.2417 - accuracy: 0.9331 - val_loss: 0.2135 - val_accuracy: 0.9388\n",
      "Epoch 3/30\n",
      "396/396 [==============================] - 1252s 3s/step - loss: 0.1810 - accuracy: 0.9459 - val_loss: 0.2019 - val_accuracy: 0.9436\n",
      "Epoch 4/30\n",
      "396/396 [==============================] - 1238s 3s/step - loss: 0.1581 - accuracy: 0.9506 - val_loss: 0.2022 - val_accuracy: 0.9434\n",
      "Epoch 5/30\n",
      "396/396 [==============================] - 1238s 3s/step - loss: 0.1355 - accuracy: 0.9569 - val_loss: 0.1826 - val_accuracy: 0.9490\n",
      "Epoch 6/30\n",
      "396/396 [==============================] - 1247s 3s/step - loss: 0.1235 - accuracy: 0.9603 - val_loss: 0.1821 - val_accuracy: 0.9503\n",
      "Epoch 7/30\n",
      "396/396 [==============================] - 1247s 3s/step - loss: 0.1154 - accuracy: 0.9631 - val_loss: 0.1873 - val_accuracy: 0.9473\n",
      "Epoch 8/30\n",
      "396/396 [==============================] - 1249s 3s/step - loss: 0.1075 - accuracy: 0.9643 - val_loss: 0.1954 - val_accuracy: 0.9483\n",
      "Epoch 9/30\n",
      "396/396 [==============================] - 1244s 3s/step - loss: 0.0992 - accuracy: 0.9667 - val_loss: 0.1923 - val_accuracy: 0.9495\n",
      "Epoch 10/30\n",
      "396/396 [==============================] - 1244s 3s/step - loss: 0.0899 - accuracy: 0.9703 - val_loss: 0.1987 - val_accuracy: 0.9500\n",
      "Epoch 11/30\n",
      "396/396 [==============================] - 1244s 3s/step - loss: 0.0858 - accuracy: 0.9712 - val_loss: 0.1916 - val_accuracy: 0.9508\n"
     ]
    }
   ],
   "source": [
    "h = model.fit(train_ds, validation_data=val_ds, epochs=30, callbacks=[es])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7e21b3-cf80-4ee0-b185-276a03af8753",
   "metadata": {},
   "source": [
    "An accuracy of 95% on the validation data is pretty good! Now we can save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0ad0245-fde9-42c8-ab86-881229cd005e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '../CNN_model'\n",
    "if os.path.isdir(model_dir) == False: os.mkdir(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e74aed0-ae5d-4398-b5ad-a0ab0c3aa6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_dir + '/efficientnet_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "897a65fd-8c45-4f6b-8952-aa81deeb2c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " efficientnetb0 (Functional)  (None, 1280)             4049571   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1280)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                40992     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               3300      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,093,863\n",
      "Trainable params: 4,051,840\n",
      "Non-trainable params: 42,023\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3820d2d-118f-4e7e-bdc9-70c8d561a765",
   "metadata": {},
   "source": [
    "Let's also try efficientnet B2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "edbffaa0-af5b-4b41-8e28-034543e59d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67cb99b1-e7c3-4c73-b006-07d9f8b031e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb2_notop.h5\n",
      "31790344/31790344 [==============================] - 2s 0us/step\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " efficientnetb2 (Functional)  (None, 1408)             7768569   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1408)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                45088     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 100)               3300      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,816,957\n",
      "Trainable params: 7,749,382\n",
      "Non-trainable params: 67,575\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "efficient_net = EfficientNetB2(\n",
    "    weights='imagenet',\n",
    "    input_shape=(100, 100, 3),\n",
    "    include_top=False,\n",
    "    pooling='max'\n",
    ")\n",
    "\n",
    "model = Sequential()\n",
    "model.add(efficient_net)\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(units = 32, activation='relu'))\n",
    "model.add(Dense(units = 100, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate = 0.0005),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "477e722f-2cab-420f-8369-244238642e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "396/396 [==============================] - 1934s 5s/step - loss: 0.7638 - accuracy: 0.8303 - val_loss: 0.3419 - val_accuracy: 0.8851\n",
      "Epoch 2/30\n",
      "396/396 [==============================] - 1889s 5s/step - loss: 0.2237 - accuracy: 0.9363 - val_loss: 0.2042 - val_accuracy: 0.9452\n",
      "Epoch 3/30\n",
      "396/396 [==============================] - 1886s 5s/step - loss: 0.1735 - accuracy: 0.9474 - val_loss: 0.1916 - val_accuracy: 0.9462\n",
      "Epoch 4/30\n",
      "396/396 [==============================] - 1892s 5s/step - loss: 0.1474 - accuracy: 0.9533 - val_loss: 0.2009 - val_accuracy: 0.9450\n",
      "Epoch 5/30\n",
      "396/396 [==============================] - 1902s 5s/step - loss: 0.1356 - accuracy: 0.9567 - val_loss: 0.1943 - val_accuracy: 0.9491\n",
      "Epoch 6/30\n",
      "396/396 [==============================] - 1894s 5s/step - loss: 0.1204 - accuracy: 0.9615 - val_loss: 0.1752 - val_accuracy: 0.9546\n",
      "Epoch 7/30\n",
      "396/396 [==============================] - 1890s 5s/step - loss: 0.1127 - accuracy: 0.9632 - val_loss: 0.1933 - val_accuracy: 0.9497\n",
      "Epoch 8/30\n",
      "396/396 [==============================] - 1883s 5s/step - loss: 0.1002 - accuracy: 0.9668 - val_loss: 0.2040 - val_accuracy: 0.9478\n",
      "Epoch 9/30\n",
      "396/396 [==============================] - 1886s 5s/step - loss: 0.0939 - accuracy: 0.9694 - val_loss: 0.2072 - val_accuracy: 0.9482\n",
      "Epoch 10/30\n",
      "396/396 [==============================] - 1888s 5s/step - loss: 0.0861 - accuracy: 0.9713 - val_loss: 0.1770 - val_accuracy: 0.9545\n",
      "Epoch 11/30\n",
      "396/396 [==============================] - 1886s 5s/step - loss: 0.0828 - accuracy: 0.9729 - val_loss: 0.2083 - val_accuracy: 0.9498\n"
     ]
    }
   ],
   "source": [
    "h = model.fit(train_ds, validation_data=val_ds, epochs=30, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220db09b-7c81-4179-9c81-080ed56ace5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
