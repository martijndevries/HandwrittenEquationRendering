{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6fab769-7359-4a8a-bd3f-0f2ff464231c",
   "metadata": {},
   "source": [
    "# EfficientNet CNN\n",
    "\n",
    "Notebook by Martijn de Vries <br>\n",
    "martijndevries91@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbb251d-5b88-4f2a-ab17-c62a89ce4559",
   "metadata": {},
   "source": [
    "NOTE: the EfficientNetB0 model that I use in this notebook has trouble saving with some version of tensorflow (probably 2.10 and higher) <br>\n",
    "I used the following fix to make it work on my machine (found in the comments here: https://github.com/keras-team/keras/issues/17199):\n",
    "\n",
    "location: lib/python3.10/site-packages/keras/applications/efficientnet.py (py3.10) <br>\n",
    "EDIT this: <br>\n",
    "<code> x = layers.Rescaling(1.0 / tf.math.sqrt(IMAGENET_STDDEV_RGB))(x) </code> <br>\n",
    "TO: <br>\n",
    "<code> x = layers.Rescaling(\n",
    "    [1.0 / math.sqrt(stddev) for stddev in IMAGENET_STDDEV_RGB]\n",
    ")(x) </code>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e447d2cd-c9aa-4237-a1b8-bafff7ed7028",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-03 10:57:30.511871: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "#tensorflow\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img, smart_resize\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from tensorflow.keras.applications import EfficientNetB0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d14b2c3-3868-4164-be57-2dfc885520d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a360b6-1216-4c74-9741-53817ce1c27a",
   "metadata": {},
   "source": [
    "I can use image_dataset_from_directory to load in the images.\n",
    "Note that even though the images are grayscale, i have to use rgb color mode because that's the format EfficientNet requires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e41751b1-f19a-4bd4-9088-50d646458e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 93020 files belonging to 100 classes.\n",
      "Using 79067 files for training.\n",
      "Found 93020 files belonging to 100 classes.\n",
      "Using 13953 files for validation.\n"
     ]
    }
   ],
   "source": [
    "train_ds = image_dataset_from_directory('../img_data/train_symbols/', image_size=(100,100),\n",
    "                                      batch_size=200, seed=123, validation_split=0.15,\n",
    "                                       subset='training', labels ='inferred', color_mode='rgb', label_mode='categorical')  \n",
    "\n",
    "val_ds = image_dataset_from_directory('../img_data/train_symbols/', image_size=(100,100),\n",
    "                                      batch_size=200, seed=123, validation_split=0.15,\n",
    "                                       subset='validation', labels ='inferred', color_mode='rgb', label_mode='categorical')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3353f501-2a7d-4626-8cbf-a855dc172db9",
   "metadata": {},
   "source": [
    "Let's look at the class names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03739932-5475-47d4-92f0-e3a5278ffc66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label_!', 'label_(', 'label_)', 'label_+', 'label_,']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.class_names[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab661af-f3d1-4f0b-9eb0-3b5882da2286",
   "metadata": {},
   "source": [
    "I'll want to save these, so I can access them when it's time to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e538f926-c23a-48c9-94b7-fc7238044ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../class_names.txt', 'w') as f:\n",
    "    for i, label in enumerate(train_ds.class_names):\n",
    "        f.write(str(i)+  ' ' + label + '\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bee089-042f-4d32-983e-13fe80355178",
   "metadata": {},
   "source": [
    "Let's also look at the images that are loaded in by image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18858742-b54d-4b01-87e7-8c0f9646eb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = []\n",
    "for images, labels in train_ds.take(1):\n",
    "    for i in range(30):\n",
    "        img_list.append(images[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ece2f1a-5b60-4633-8e91-39f2ec964b91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([100, 100, 3])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_list[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "49053b1c-4992-46f3-9809-294971a4c149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img_list[3][:,:,0], cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801dcced-9ca1-4e44-a0da-8215cbc4e9a0",
   "metadata": {},
   "source": [
    "That looks alright! It's important that the size of the symbols is roughtly similar to the symbol size out of the pre-processing pipeline.\n",
    "\n",
    "Now we can set up the neural network. For optimal performance, I'll use the EfficientNetB0 network, and re-train it using my data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8c966a2-2f01-4348-ba55-eee6f746d504",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df7b8990-e181-425f-9e7a-316f186be278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " efficientnetb0 (Functional)  (None, 1280)             4049571   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1280)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                40992     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               3300      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,093,863\n",
      "Trainable params: 4,051,840\n",
      "Non-trainable params: 42,023\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "efficient_net = EfficientNetB0(\n",
    "    weights='imagenet',\n",
    "    input_shape=(100, 100, 3),\n",
    "    include_top=False,\n",
    "    pooling='max'\n",
    ")\n",
    "\n",
    "model = Sequential()\n",
    "model.add(efficient_net)\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(units = 32, activation='relu'))\n",
    "model.add(Dense(units = 100, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate = 0.0005),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df9ff22-c268-41bb-b185-7982472d0b3f",
   "metadata": {},
   "source": [
    "Now we can fit the model. NB: on my local machine this took about 4hrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc6d7cb9-feb3-4ce1-a8e1-b24c41dfcbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "396/396 [==============================] - 1311s 3s/step - loss: 0.8973 - accuracy: 0.8047 - val_loss: 0.3688 - val_accuracy: 0.9106\n",
      "Epoch 2/30\n",
      "396/396 [==============================] - 1269s 3s/step - loss: 0.2417 - accuracy: 0.9331 - val_loss: 0.2135 - val_accuracy: 0.9388\n",
      "Epoch 3/30\n",
      "396/396 [==============================] - 1252s 3s/step - loss: 0.1810 - accuracy: 0.9459 - val_loss: 0.2019 - val_accuracy: 0.9436\n",
      "Epoch 4/30\n",
      "396/396 [==============================] - 1238s 3s/step - loss: 0.1581 - accuracy: 0.9506 - val_loss: 0.2022 - val_accuracy: 0.9434\n",
      "Epoch 5/30\n",
      "396/396 [==============================] - 1238s 3s/step - loss: 0.1355 - accuracy: 0.9569 - val_loss: 0.1826 - val_accuracy: 0.9490\n",
      "Epoch 6/30\n",
      "396/396 [==============================] - 1247s 3s/step - loss: 0.1235 - accuracy: 0.9603 - val_loss: 0.1821 - val_accuracy: 0.9503\n",
      "Epoch 7/30\n",
      "396/396 [==============================] - 1247s 3s/step - loss: 0.1154 - accuracy: 0.9631 - val_loss: 0.1873 - val_accuracy: 0.9473\n",
      "Epoch 8/30\n",
      "396/396 [==============================] - 1249s 3s/step - loss: 0.1075 - accuracy: 0.9643 - val_loss: 0.1954 - val_accuracy: 0.9483\n",
      "Epoch 9/30\n",
      "396/396 [==============================] - 1244s 3s/step - loss: 0.0992 - accuracy: 0.9667 - val_loss: 0.1923 - val_accuracy: 0.9495\n",
      "Epoch 10/30\n",
      "396/396 [==============================] - 1244s 3s/step - loss: 0.0899 - accuracy: 0.9703 - val_loss: 0.1987 - val_accuracy: 0.9500\n",
      "Epoch 11/30\n",
      "396/396 [==============================] - 1244s 3s/step - loss: 0.0858 - accuracy: 0.9712 - val_loss: 0.1916 - val_accuracy: 0.9508\n"
     ]
    }
   ],
   "source": [
    "h = model.fit(train_ds, validation_data=val_ds, epochs=30, callbacks=[es])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7e21b3-cf80-4ee0-b185-276a03af8753",
   "metadata": {},
   "source": [
    "That losk pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0ad0245-fde9-42c8-ab86-881229cd005e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '../CNN_model'\n",
    "if os.path.isdir(model_dir) == False: os.mkdir(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e74aed0-ae5d-4398-b5ad-a0ab0c3aa6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_dir + '/efficientnet_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "897a65fd-8c45-4f6b-8952-aa81deeb2c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " efficientnetb0 (Functional)  (None, 1280)             4049571   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1280)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                40992     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               3300      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,093,863\n",
      "Trainable params: 4,051,840\n",
      "Non-trainable params: 42,023\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3820d2d-118f-4e7e-bdc9-70c8d561a765",
   "metadata": {},
   "source": [
    "Let's also try efficientnet B2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "edbffaa0-af5b-4b41-8e28-034543e59d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67cb99b1-e7c3-4c73-b006-07d9f8b031e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb2_notop.h5\n",
      "31790344/31790344 [==============================] - 2s 0us/step\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " efficientnetb2 (Functional)  (None, 1408)             7768569   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1408)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                45088     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 100)               3300      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,816,957\n",
      "Trainable params: 7,749,382\n",
      "Non-trainable params: 67,575\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "efficient_net = EfficientNetB2(\n",
    "    weights='imagenet',\n",
    "    input_shape=(100, 100, 3),\n",
    "    include_top=False,\n",
    "    pooling='max'\n",
    ")\n",
    "\n",
    "model = Sequential()\n",
    "model.add(efficient_net)\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(units = 32, activation='relu'))\n",
    "model.add(Dense(units = 100, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate = 0.0005),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "477e722f-2cab-420f-8369-244238642e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "396/396 [==============================] - 1934s 5s/step - loss: 0.7638 - accuracy: 0.8303 - val_loss: 0.3419 - val_accuracy: 0.8851\n",
      "Epoch 2/30\n",
      "396/396 [==============================] - 1889s 5s/step - loss: 0.2237 - accuracy: 0.9363 - val_loss: 0.2042 - val_accuracy: 0.9452\n",
      "Epoch 3/30\n",
      "396/396 [==============================] - 1886s 5s/step - loss: 0.1735 - accuracy: 0.9474 - val_loss: 0.1916 - val_accuracy: 0.9462\n",
      "Epoch 4/30\n",
      "396/396 [==============================] - 1892s 5s/step - loss: 0.1474 - accuracy: 0.9533 - val_loss: 0.2009 - val_accuracy: 0.9450\n",
      "Epoch 5/30\n",
      "396/396 [==============================] - 1902s 5s/step - loss: 0.1356 - accuracy: 0.9567 - val_loss: 0.1943 - val_accuracy: 0.9491\n",
      "Epoch 6/30\n",
      "396/396 [==============================] - 1894s 5s/step - loss: 0.1204 - accuracy: 0.9615 - val_loss: 0.1752 - val_accuracy: 0.9546\n",
      "Epoch 7/30\n",
      "396/396 [==============================] - 1890s 5s/step - loss: 0.1127 - accuracy: 0.9632 - val_loss: 0.1933 - val_accuracy: 0.9497\n",
      "Epoch 8/30\n",
      "396/396 [==============================] - 1883s 5s/step - loss: 0.1002 - accuracy: 0.9668 - val_loss: 0.2040 - val_accuracy: 0.9478\n",
      "Epoch 9/30\n",
      "396/396 [==============================] - 1886s 5s/step - loss: 0.0939 - accuracy: 0.9694 - val_loss: 0.2072 - val_accuracy: 0.9482\n",
      "Epoch 10/30\n",
      "396/396 [==============================] - 1888s 5s/step - loss: 0.0861 - accuracy: 0.9713 - val_loss: 0.1770 - val_accuracy: 0.9545\n",
      "Epoch 11/30\n",
      "396/396 [==============================] - 1886s 5s/step - loss: 0.0828 - accuracy: 0.9729 - val_loss: 0.2083 - val_accuracy: 0.9498\n"
     ]
    }
   ],
   "source": [
    "h = model.fit(train_ds, validation_data=val_ds, epochs=30, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220db09b-7c81-4179-9c81-080ed56ace5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
