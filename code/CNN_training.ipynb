{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6fab769-7359-4a8a-bd3f-0f2ff464231c",
   "metadata": {},
   "source": [
    "# EfficientNet CNN\n",
    "\n",
    "Notebook by Martijn de Vries <br>\n",
    "martijndevries91@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbb251d-5b88-4f2a-ab17-c62a89ce4559",
   "metadata": {},
   "source": [
    "NOTE: the EfficientNetB0 model that I use in this notebook has trouble saving with some version of tensorflow (versions 2.10 and higher) <br>\n",
    "I used the following fix to make it work on my machine (found in the comments here: https://github.com/keras-team/keras/issues/17199):\n",
    "\n",
    "location: lib/python3.10/site-packages/keras/applications/efficientnet.py (py3.10) <br>\n",
    "EDIT this: <br>\n",
    "<code> x = layers.Rescaling(1.0 / tf.math.sqrt(IMAGENET_STDDEV_RGB))(x) </code> <br>\n",
    "TO: <br>\n",
    "<code> x = layers.Rescaling(\n",
    "    [1.0 / math.sqrt(stddev) for stddev in IMAGENET_STDDEV_RGB]\n",
    ")(x) </code>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e447d2cd-c9aa-4237-a1b8-bafff7ed7028",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-08 09:07:59.105923: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "#tensorflow\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img, smart_resize\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from tensorflow.keras.applications import EfficientNetB0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d14b2c3-3868-4164-be57-2dfc885520d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a360b6-1216-4c74-9741-53817ce1c27a",
   "metadata": {},
   "source": [
    "I can use image_dataset_from_directory to load in the images.\n",
    "Note that even though the images are grayscale, i have to use rgb color mode because that's the format EfficientNet requires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e41751b1-f19a-4bd4-9088-50d646458e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 83938 files belonging to 100 classes.\n",
      "Using 75545 files for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-08 09:08:17.474089: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 83938 files belonging to 100 classes.\n",
      "Using 8393 files for validation.\n"
     ]
    }
   ],
   "source": [
    "train_ds = image_dataset_from_directory('../img_data/train_symbols/', image_size=(100,100),\n",
    "                                      batch_size=200, seed=123, validation_split=0.10,\n",
    "                                       subset='training', labels ='inferred', color_mode='rgb', label_mode='categorical')  \n",
    "\n",
    "val_ds = image_dataset_from_directory('../img_data/train_symbols/', image_size=(100,100),\n",
    "                                      batch_size=200, seed=123, validation_split=0.10,\n",
    "                                       subset='validation', labels ='inferred', color_mode='rgb', label_mode='categorical')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3353f501-2a7d-4626-8cbf-a855dc172db9",
   "metadata": {},
   "source": [
    "Let's look at the class names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03739932-5475-47d4-92f0-e3a5278ffc66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label_!', 'label_(', 'label_)', 'label_+', 'label_,']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.class_names[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab661af-f3d1-4f0b-9eb0-3b5882da2286",
   "metadata": {},
   "source": [
    "I'll want to save these, so I can access them when it's time to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e538f926-c23a-48c9-94b7-fc7238044ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../class_names.txt', 'w') as f:\n",
    "    for i, label in enumerate(train_ds.class_names):\n",
    "        f.write(str(i)+  ' ' + label + '\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bee089-042f-4d32-983e-13fe80355178",
   "metadata": {},
   "source": [
    "Let's also look at the images that are loaded in by image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18858742-b54d-4b01-87e7-8c0f9646eb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = []\n",
    "for images, labels in train_ds.take(1):\n",
    "    for i in range(30):\n",
    "        img_list.append(images[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ece2f1a-5b60-4633-8e91-39f2ec964b91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([100, 100, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_list[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49053b1c-4992-46f3-9809-294971a4c149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaEAAAGgCAYAAAAD9NhnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAj/0lEQVR4nO3df1SUZf7/8fcIOoDCuFKAKCK2lKZ1dMFslYK1pJNW27prlj/LNjU1JTupLFasKZjnZBzbtHRbdbfITmVl7VaQGeWx3YxCTQsrSUklsjwzmAgG1/cPP97fuQZLBmbmGuD5OGfOud73Ncx9eWe8vO7rnvu2KaWUAABgQCfTAwAAdFyEEADAGEIIAGAMIQQAMIYQAgAYQwgBAIwhhAAAxhBCAABjCCEAgDGEEADAGL+F0OrVqyUpKUnCwsIkJSVF3n//fX/tCgDQRoX640Off/55ycrKktWrV8uIESPkqaeekuuvv1727dsnffr0+cWfbWxslCNHjkhkZKTYbDZ/DA8A4EdKKampqZH4+Hjp1Ok8cx3lB1dccYWaOXOmtq1///5q0aJF5/3ZyspKJSK8ePHixauNvyorK8/7O9/nM6H6+nopLS2VRYsWadszMzNlx44dTd5fV1cndXV1Vq3+76belZWVEhUV5evhAQD8zOVySUJCgkRGRp73vT4PoWPHjklDQ4PExsZq22NjY6WqqqrJ+/Pz8+Wvf/1rk+1RUVGEEAC0Yc1ZUvHbhQmeO1dKnXNA2dnZ4nQ6rVdlZaW/hgQACDI+nwldcMEFEhIS0mTWU11d3WR2JCJit9vFbrf7ehgAgDbA5zOhLl26SEpKihQXF2vbi4uLZfjw4b7eHQCgDfPLJdrz58+XyZMnS2pqqvz2t7+VtWvXyqFDh2TmzJn+2B0AoI3ySwiNHz9evv/+e1myZIkcPXpUBg0aJP/5z38kMTHRH7sDALRRNnX2mugg4XK5xOFwiNPp5Oo4AGiDvPk9zr3jAADGEEIAAGMIIQCAMYQQAMAYQggAYAwhBAAwhhACABhDCAEAjCGEAADGEEIAAGMIIQCAMYQQAMAYQggAYAwhBAAwhhACABhDCAEAjCGEAADGEEIAAGMIIQCAMYQQAMAYQggAYAwhBAAwhhACABhDCAEAjCGEAADGEEIAAGMIIQCAMYQQAMAYQggAYAwhBAAwhhACABhDCAEAjCGEAADGEEIAAGNCTQ8ACGb19fVW+7///a/Wt2HDBq0+cOCAVjc0NLRon0lJSVp9yy23aPWoUaOstt1ub9E+gGDBTAgAYAwhBAAwhhACABhjU0op04Nw53K5xOFwiNPplKioKNPDQQdTV1en1ffdd5/V3rhxo9Z34sSJgIwpLCxMq++44w6r/dhjj2l9rBEhGHjze5yZEADAGEIIAGAMIQQAMIbvCaFDKy8v1+rp06dr9XvvvRfI4ZzTqVOntPrvf/+71U5OTtb6srKytNpms/ltXIAvMBMCABhDCAEAjOF0HDqc1157zWrPnTtX6/v6668DPBrvnT592mrn5ORofT169NDqqVOnBmRMQEsxEwIAGEMIAQCMIYQAAMawJoR2b+vWrVrtftub77//PtDD8ana2lqtfvjhh7X6mmuu0erevXv7fUyAN5gJAQCMIYQAAMYQQgAAY1gTQruze/durZ48ebJWt/V1oF9SUVGh1Zs3b9Zqz+9FAaYxEwIAGEMIAQCMIYQAAMbweG+0C4cOHbLaY8aM0fo+/fTTQA9HREQuvfRSqx0REaH1HTt2TKv9dc+6xMRErd6/f7/V7tKli1/2CfB4bwBAm0AIAQCM4RJttEmet6u5/fbbrfbevXsDMobIyEitzsvL0+rx48db7fDwcK2vsrJSq2+66Sar/eWXX/pqiHLw4EGtfuutt6z2jTfe6LP9AC3FTAgAYAwhBAAwxqsQys/Pl6FDh0pkZKTExMTIzTffLOXl5dp7lFKSm5sr8fHxEh4eLhkZGQE7PQIAaFu8WhMqKSmR2bNny9ChQ+Wnn36SnJwcyczMlH379knXrl1FRGTFihWycuVK2bBhg1x88cWydOlSGTVqlJSXlzc5hw4018mTJ7X6z3/+s1Zv27bN72PwfAzCU089pdWjR49u9mcNGDBAq2fNmmW158+f34LRNc+WLVusNmtCCAZehdCbb76p1evXr5eYmBgpLS2Vq6++WpRSUlBQIDk5OTJ27FgREdm4caPExsZKYWGhzJgxo8ln1tXVSV1dnVW7XK6W/DkAAG1Qq9aEnE6niIj06NFDRM7cPLGqqkoyMzOt99jtdklPT5cdO3ac8zPy8/PF4XBYr4SEhNYMCQDQhrQ4hJRSMn/+fElLS5NBgwaJiEhVVZWIiMTGxmrvjY2Ntfo8ZWdni9PptF6el64CANqvFn9PaM6cObJ7927Zvn17kz6bzabVSqkm286y2+1it9tbOgx0EK+++qpWv/TSSwHZ74UXXmi1CwsLtb6rrrrKZ/uZNm2a1X7ggQe0vh9//NFn+/nss8+sdn19vdbHbXxgQotmQvfcc49s2bJFtm3bpi3WxsXFiYg0mfVUV1c3mR0BAOBVCCmlZM6cObJ582Z55513JCkpSetPSkqSuLg4KS4utrbV19dLSUmJDB8+3DcjBgC0G16djps9e7YUFhbKq6++KpGRkdaMx+FwSHh4uNhsNsnKypK8vDxJTk6W5ORkycvLk4iICJkwYYJf/gBonzyvkly8eLFWe55K8hXP2+ssXbrUavvy9Jsnh8NhtVNSUrS+9957z2f7OXsxkYhITU2N1hcdHe2z/QDN5VUIrVmzRkREMjIytO3r16+37t21YMECqa2tlVmzZsnx48dl2LBhUlRUxHeEAABNeBVCzXn0kM1mk9zcXMnNzW3pmAAAHQT3jgMAGMOjHBCUli9frtUHDhzwy346d+6s1Y888ohWe94eKBD69eun1b5cE3JfS/PXuhrgDWZCAABjCCEAgDGEEADAGNaEEDS++uorq+15ixx/mTRpklZPnz5dqzt1Cvy/037uFle+0NjYaLUbGhr8th+guZgJAQCMIYQAAMZwOg5B4/HHH7fahw4d8tt++vbta7Ufe+wxrS8Y7uj+xRdf+O2zIyIirPbZpyEDJjETAgAYQwgBAIwhhAAAxrAmBGM8n8q7atUqq92cm+U2V2io/tf84Ycfttruj1AIFv5cE3K/m323bt38th+guZgJAQCMIYQAAMYQQgAAY1gTQsC4P1paRCQ7O1urfbkO5G7gwIFaff311/tlP6dOndLqsLCwZv+s+7Gprq722Zg8xcfHW23Px1gAJjATAgAYQwgBAIwhhAAAxrAmhIDZtWuXVpeWlvplP57fC3rwwQe1Ojo62if7KSoq0ur77rtPq59++mmrPXToUK3P83EN7777rtX219qYiMgll1zit88GWoKZEADAGEIIAGAMp+MQMC+88IJW19bW+mU/EydO1Oqbb77ZJ5/7yiuvaLXnU1i/++47rf7Tn/5ktV988UWt77LLLtPq1atX+2CE53fFFVcEZD9AczETAgAYQwgBAIwhhAAAxrAmBL86ceKE1X722Wf9tp/Y2FirvXDhQq2vU6eW/1vL/TLs860BeaqsrLTaN910k9Y3d+5crX7vvfdaOsRf5P44bxGR9PR0v+wHaClmQgAAYwghAIAxhBAAwBjWhOBX7t8NOn78uN/2k5qaarWTk5Nb/Dm/9F2g860B/ZJvv/1Wq3Nyclr8Wd6YOnWqVkdFRQVkv0BzMRMCABhDCAEAjOF0HHzq9OnTWl1YWBiQ/d5+++1W2/Mu2r/k6NGjWr148WKtbs0pOBM8T7fdeeedhkYCNA8zIQCAMYQQAMAYQggAYAxrQvApzzWWL7/80i/76dWrl1bfcMMNLfoc99v9iIisWrVKq//whz9YbZfL1aJ9BNJ1112n1YMGDTI0EqB5mAkBAIwhhAAAxhBCAABjWBOCT3399dda7blG5CuTJk3S6rCwsBZ9judjHkaOHKnVL7/8stWeMGGC1ud5Kx4T3G9XJCKybt06rbbb7YEcDuA1ZkIAAGMIIQCAMZyOg09t3bpVq+vq6nzyuV26dNHq66+/3iefez4ZGRlW2/MWRGPHjtVqp9MZiCHJ4MGDrfamTZu0PofDEZAxAL7CTAgAYAwhBAAwhhACABjDmhBaRSml1Zs3b/bLfnr27KnViYmJftmPJ/dLuD0v3/Zc/5o1a5ZWl5aWWu2GhoZm79PzsurRo0dr9T/+8Q+r3b1792Z/LhCMmAkBAIwhhAAAxhBCAABjWBNCq3z++edavXfvXr/sx3MNKD4+3i/78UZKSopWv/7661q9fft2q/3BBx9ofZ6PDe/WrZvV9nwcg/t3lTzfC7R1zIQAAMYQQgAAYwghAIAxrAmhVV588UWt9vzekK+MGjVKqz3vJRcMLrzwQq12fzS4exvA/8dMCABgDCEEADCG03Hw2qlTp6x2UVFRQPY5bty4gOwHQGAxEwIAGEMIAQCMaVUI5efni81mk6ysLGubUkpyc3MlPj5ewsPDJSMjw2/fogcAtG0tXhPauXOnrF27Vi6//HJt+4oVK2TlypWyYcMGufjii2Xp0qUyatQoKS8vl8jIyFYPGOYdOXLEah88eNAv+xg0aJBWX3LJJX7ZDwCzWjQTOnHihEycOFHWrVsnv/rVr6ztSikpKCiQnJwcGTt2rAwaNEg2btwoJ0+elMLCwnN+Vl1dnbhcLu0FAOgYWhRCs2fPljFjxsi1116rba+oqJCqqirJzMy0ttntdklPT5cdO3ac87Py8/PF4XBYr4SEhJYMCQDQBnkdQps2bZKPP/5Y8vPzm/RVVVWJiEhsbKy2PTY21urzlJ2dLU6n03pVVlZ6OyQAQBvl1ZpQZWWlzJs3T4qKiiQsLOxn32ez2bRaKdVk21l2u73J44wR3Nz/QfFz/7horfHjx/vlcwEEF69mQqWlpVJdXS0pKSkSGhoqoaGhUlJSIqtWrZLQ0FBrBuT5i6m6urrJ7AgAAK9C6JprrpE9e/ZIWVmZ9UpNTZWJEydKWVmZ9OvXT+Li4qS4uNj6mfr6eikpKZHhw4f7fPAAgLbNq9NxkZGRTS6d7dq1q0RHR1vbs7KyJC8vT5KTkyU5OVny8vIkIiJCJkyY4LtRw6jTp0+fs91a7qdlR44c6bPPBRC8fH7vuAULFkhtba3MmjVLjh8/LsOGDZOioiK+IwQAaKLVIfTuu+9qtc1mk9zcXMnNzW3tRwMA2jnuHQcAMIZHOSBoREdHW23Pp5QCaJ+YCQEAjCGEAADGEEIAAGNYE0LQiIqKstoOh8PgSAAECjMhAIAxhBAAwBhOxyFodO/e3WpzOg7oGJgJAQCMIYQAAMYQQgAAY1gTQtDo1auX1eZpu0DHwEwIAGAMIQQAMIYQAgAYw5oQgobno+MBtH/MhAAAxhBCAABjCCEAgDGsCSFoDBw40PQQAAQYMyEAgDGEEADAGE7HIWhwiTbQ8TATAgAYQwgBAIwhhAAAxrAmhKBx0UUXmR4CgABjJgQAMIYQAgAYQwgBAIxhTQhBQylleggAAoyZEADAGEIIAGAMp+MQNJxOp9WOiYkxOBIAgcJMCABgDCEEADCGEAIAGMOaEIKGy+Wy2qwJAR0DMyEAgDGEEADAGEIIAGAMa0IIGjU1NaaHACDAmAkBAIwhhAAAxnA6DkEjNJS/jkBHw0wIAGAMIQQAMIYQAgAYw0l4BA273W56CAACjJkQAMAYQggAYAwhBAAwhjUhBA3WhICOh5kQAMAYQggAYAwhBAAwhjUhBI2QkBDTQwAQYMyEAADGEEIAAGM4HYegceDAAavdu3dvgyMBECjMhAAAxhBCAABjvA6hw4cPy6RJkyQ6OloiIiJk8ODBUlpaavUrpSQ3N1fi4+MlPDxcMjIyZO/evT4dNACgffAqhI4fPy4jRoyQzp07yxtvvCH79u2TRx99VLp37269Z8WKFbJy5Ur529/+Jjt37pS4uDgZNWqU1NTU+HrsaGd27dplvQB0DF5dmPDII49IQkKCrF+/3trWt29fq62UkoKCAsnJyZGxY8eKiMjGjRslNjZWCgsLZcaMGU0+s66uTurq6qza5XJ5+2cAALRRXs2EtmzZIqmpqTJu3DiJiYmRIUOGyLp166z+iooKqaqqkszMTGub3W6X9PR02bFjxzk/Mz8/XxwOh/VKSEho4R8FANDWeBVCBw4ckDVr1khycrK89dZbMnPmTJk7d67885//FBGRqqoqERGJjY3Vfi42Ntbq85SdnS1Op9N6VVZWtuTPAQBog7w6HdfY2CipqamSl5cnIiJDhgyRvXv3ypo1a2TKlCnW+2w2m/ZzSqkm286y2+3cwh8iIqwFAR2QVzOhnj17yqWXXqptGzBggBw6dEhEROLi4kREmsx6qqurm8yOAADwKoRGjBgh5eXl2rb9+/dLYmKiiIgkJSVJXFycFBcXW/319fVSUlIiw4cP98FwAQDtiVen4+69914ZPny45OXlyS233CIffvihrF27VtauXSsiZ07DZWVlSV5eniQnJ0tycrLk5eVJRESETJgwwS9/ALQfX3zxhdVubGzU+jp14nvVQHvkVQgNHTpUXn75ZcnOzpYlS5ZIUlKSFBQUyMSJE633LFiwQGpra2XWrFly/PhxGTZsmBQVFUlkZKTPBw8AaNtsSillehDuXC6XOBwOcTqdEhUVZXo4OIeSkhKrnZGR4bPPvfrqq632tm3btD5mQkDb4c3vcf7PBgAYw6McEDROnjxptU+dOqX1RUREBHo4AAKAmRAAwBhCCABgDCEEADCGNSEEDfd1INaEgI6BmRAAwBhCCABgDKfjEDTq6+uttvuDDgG0X8yEAADGEEIAAGMIIQCAMawJIWg4nc5ztkXOPFARQPvDTAgAYAwhBAAwhhACABjDmhCCxnfffWe1P/roI62vf//+gR4OgABgJgQAMIYQAgAYQwgBAIxhTQhei4yMPGdbRKSmpqbFn9vY2Gi1n332Wa1v0qRJLf5cAMGLmRAAwBhCCABgDKfj4LW+ffta7cTERK3v008/9ck+3nnnHa3+5ptvtLp3794+2Q8As5gJAQCMIYQAAMYQQgAAY1gTgtd69OhhtdPS0rQ+X60JuT/qW0Tktdde0+q7777bJ/sBYBYzIQCAMYQQAMAYQggAYAxrQmiVKVOmaPWTTz7pl/1s2bJFq6dPn67VISEhftkvzPrhhx+0+vTp01Y7NjY20MOBHzATAgAYQwgBAIzhdBxa5corr9TqXr16afXhw4d9sp/9+/dr9ZEjR7Q6ISHBJ/tB4LnfPd3zEv9p06ZptVLKar/xxhtaX0xMjB9GB39jJgQAMIYQAgAYQwgBAIxhTQitYrPZtPqPf/yjVq9atcon+zl48KBWv/nmm1p91113+WQ/CLyXX37Zas+dO1fr81z7c+f539zzabzdunXzwejgb8yEAADGEEIAAGMIIQCAMTblfuF9EHC5XOJwOMTpdEpUVJTp4cBLxcXFWj169Gir/dNPP/lsP56PFf/444+ttvujJmDeiRMntNpznXDx4sVW25tfR563asrPz9fq+++/v9mfBd/y5vc4MyEAgDGEEADAGE7Hwae+/fZbrc7IyLDan3/+ud/2O3nyZKu9bt06rc9ut/ttvzi/qVOnavVzzz2n1e53xm6N8PBwrT558qRPPhfe43QcAKBNIIQAAMYQQgAAY7htD3zK82mX7usB2dnZftvviy++aLXHjBmj9Y0fP95v+8X53XDDDVrt/t9KxHdrQrW1tT75HAQWMyEAgDGEEADAGEIIAGAM3xOCX7nfsmXw4MFa31dffeWXfUZHR2v1K6+8otVpaWl+2S/OzeVyaXV6erpWl5WV+WW/QfarrUPhe0IAgDaBEAIAGEMIAQCM4XtC8Cv3RywvX75c63O/35uIyKlTp3yyz++//16r77jjDq3etGmT1U5JSfHJPju6mpoaq/3JJ59ofQ899JBW+2sNCG0TMyEAgDGEEADAGC7RRsB4nm6bPn26Vv/rX/8KyDji4+OttuepojvvvFOrPZ/e2VFVV1drtfspTRH9v91nn32m9f3444/+G9gvCLJfbR0Kl2gDANoEQggAYIxXIfTTTz/J4sWLJSkpScLDw6Vfv36yZMkSaWxstN6jlJLc3FyJj4+X8PBwycjIkL179/p84ACAts+rS7QfeeQRefLJJ2Xjxo0ycOBA+eijj+SOO+4Qh8Mh8+bNExGRFStWyMqVK2XDhg1y8cUXy9KlS2XUqFFSXl4ukZGRfvlDoG0ICwvT6ieeeEKrd+/ebbV37drlt3EcOXLEas+ZM+dnxyAiMnPmTKvdt29frc/98vO2wv0WOocOHdL6SktLtbqwsNBqb926VetraGjww+hap0uXLqaHgBbwKoQ++OAD+f3vf289r6Vv377y3HPPyUcffSQiZ2ZBBQUFkpOTI2PHjhURkY0bN0psbKwUFhbKjBkzmnxmXV2d1NXVWbXnfaYAAO2XV6fj0tLSZOvWrbJ//34ROfOv1e3bt8vo0aNFRKSiokKqqqokMzPT+hm73S7p6emyY8eOc35mfn6+OBwO65WQkNDSPwsAoI3xaia0cOFCcTqd0r9/fwkJCZGGhgZZtmyZ3HbbbSIiUlVVJSJNn64ZGxsrBw8ePOdnZmdny/z5863a5XIRRADQQXgVQs8//7w888wzUlhYKAMHDpSysjLJysqS+Ph47THONptN+zmlVJNtZ9ntdrHb7S0YOto6zzXC9evXW+0bb7xR6zt8+LBfxuD5aGnPdSr378N4rgl53vLH87HiI0eOtNq+XD9yX4/xPC47d+7U6n//+99a7X5LnW+++UbrO3bsmK+GGBAOh0OrH330UUMjQWt4FUL333+/LFq0SG699VYREbnsssvk4MGDkp+fL1OnTpW4uDgROTMj6tmzp/Vz1dXVTWZHAAB4tSZ08uRJ6dRJ/5GQkBDrEu2kpCSJi4uT4uJiq7++vl5KSkpk+PDhPhguAKA98WomdOONN8qyZcukT58+MnDgQPnkk09k5cqVMm3aNBE5cxouKytL8vLyJDk5WZKTkyUvL08iIiJkwoQJfvkDoP0YMmSI1fY8tXLXXXdptftdm/3J/Y7cnnfn9rykee3atVrtfsufmJgYre/CCy/UavfLi2tra7U+z9veHD161Gq7X1naHnneNmnAgAFWe9WqVVrf7373u4CMCb7lVQg9/vjj8sADD8isWbOkurpa4uPjZcaMGfLggw9a71mwYIHU1tbKrFmz5Pjx4zJs2DApKiriO0IAgCa8CqHIyEgpKCiQgoKCn32PzWaT3Nxcyc3NbeXQAADtHfeOAwAYw5NVEZTGjx+v1Z4XxHjefeP48eN+H5O33C+ldl/HOVeNMzwvZf/rX/+q1e5P4/VcV0PbxEwIAGAMIQQAMIYQAgAYw5oQ2oRx48Zptef9Bd2/R7Rv3z6tz/15Vwg8z1t2ed495aqrrrLaK1as0Po8b5WE9oeZEADAGEIIAGAMp+PQJl155ZVaXVRUZLWXLVum9XneGRv+5XkK7e6779Zqzzuk//rXv7banTt39tu4EJyYCQEAjCGEAADGEEIAAGNsSillehDuXC6XOBwOcTqdEhUVZXo4aAe2bNmi1UuWLLHa5eXlWt+JEycCMqa2oEePHla7T58+Wl9aWppWu99myXO9LjSUpeeOxpvf48yEAADGEEIAAGMIIQCAMawJocP54YcfrPZXX32l9e3cuVOrX3/9dau9bds2re/UqVN+GJ1/de3a1WqPHDlS6xszZoxWu6/tJCYman3du3f3/eDQbrAmBABoEwghAIAxhBAAwBjWhIBm8vwOkeca0dtvv63VFRUVVtvpdGp9dXV1Wu3+v6Hno8w9v2fjvq7Tr18/re+iiy7S6ssvv1yr3b/fEx4eLoA/sCYEAGgTCCEAgDGcjgMCwPPprvX19Vrt/r9hSEiI1ne+Ggg2nI4DALQJhBAAwBhCCABgDPdYBwLA87LrsLAwQyMBggszIQCAMYQQAMAYQggAYAwhBAAwhhACABhDCAEAjCGEAADGEEIAAGMIIQCAMYQQAMAYQggAYAwhBAAwhhACABhDCAEAjCGEAADGEEIAAGMIIQCAMYQQAMAYQggAYAwhBAAwhhACABhDCAEAjCGEAADGEEIAAGMIIQCAMYQQAMAYQggAYAwhBAAwhhACABhDCAEAjCGEAADGEEIAAGMIIQCAMYQQAMAYQggAYEyo6QF4UkqJiIjL5TI8EgBAS5z9/X329/kvCboQqqmpERGRhIQEwyMBALRGTU2NOByOX3yPTTUnqgKosbFRjhw5Ikop6dOnj1RWVkpUVJTpYQUtl8slCQkJHKfz4Dg1D8epeThOv0wpJTU1NRIfHy+dOv3yqk/QzYQ6deokvXv3tqZzUVFR/EduBo5T83Ccmofj1Dwcp593vhnQWVyYAAAwhhACABgTtCFkt9vloYceErvdbnooQY3j1Dwcp+bhODUPx8l3gu7CBABAxxG0MyEAQPtHCAEAjCGEAADGEEIAAGMIIQCAMUEbQqtXr5akpCQJCwuTlJQUef/9900PyZj8/HwZOnSoREZGSkxMjNx8881SXl6uvUcpJbm5uRIfHy/h4eGSkZEhe/fuNTTi4JCfny82m02ysrKsbRynMw4fPiyTJk2S6OhoiYiIkMGDB0tpaanVz3ES+emnn2Tx4sWSlJQk4eHh0q9fP1myZIk0NjZa7+E4+YAKQps2bVKdO3dW69atU/v27VPz5s1TXbt2VQcPHjQ9NCOuu+46tX79evXpp5+qsrIyNWbMGNWnTx914sQJ6z3Lly9XkZGR6qWXXlJ79uxR48ePVz179lQul8vgyM358MMPVd++fdXll1+u5s2bZ23nOCn1ww8/qMTERHX77ber//3vf6qiokK9/fbb6ssvv7Tew3FSaunSpSo6Olq9/vrrqqKiQr3wwguqW7duqqCgwHoPx6n1gjKErrjiCjVz5kxtW//+/dWiRYsMjSi4VFdXKxFRJSUlSimlGhsbVVxcnFq+fLn1nlOnTimHw6GefPJJU8M0pqamRiUnJ6vi4mKVnp5uhRDH6YyFCxeqtLS0n+3nOJ0xZswYNW3aNG3b2LFj1aRJk5RSHCdfCbrTcfX19VJaWiqZmZna9szMTNmxY4ehUQUXp9MpIiI9evQQEZGKigqpqqrSjpndbpf09PQOecxmz54tY8aMkWuvvVbbznE6Y8uWLZKamirjxo2TmJgYGTJkiKxbt87q5zidkZaWJlu3bpX9+/eLiMiuXbtk+/btMnr0aBHhOPlK0N1F+9ixY9LQ0CCxsbHa9tjYWKmqqjI0quChlJL58+dLWlqaDBo0SETEOi7nOmYHDx4M+BhN2rRpk3z88ceyc+fOJn0cpzMOHDgga9askfnz58tf/vIX+fDDD2Xu3Llit9tlypQpHKf/s3DhQnE6ndK/f38JCQmRhoYGWbZsmdx2220iwt8nXwm6EDrLZrNptVKqybaOaM6cObJ7927Zvn17k76OfswqKytl3rx5UlRUJGFhYT/7vo5+nBobGyU1NVXy8vJERGTIkCGyd+9eWbNmjUyZMsV6X0c/Ts8//7w888wzUlhYKAMHDpSysjLJysqS+Ph4mTp1qvW+jn6cWivoTsddcMEFEhIS0mTWU11d3eRfHB3NPffcI1u2bJFt27ZJ7969re1xcXEiIh3+mJWWlkp1dbWkpKRIaGiohIaGSklJiaxatUpCQ0OtY9HRj1PPnj3l0ksv1bYNGDBADh06JCL8fTrr/vvvl0WLFsmtt94ql112mUyePFnuvfdeyc/PFxGOk68EXQh16dJFUlJSpLi4WNteXFwsw4cPNzQqs5RSMmfOHNm8ebO88847kpSUpPUnJSVJXFycdszq6+ulpKSkQx2za665Rvbs2SNlZWXWKzU1VSZOnChlZWXSr18/jpOIjBgxoskl/vv375fExEQR4e/TWSdPnmzyVNCQkBDrEm2Ok48YvCjiZ529RPvpp59W+/btU1lZWapr167q66+/Nj00I+6++27lcDjUu+++q44ePWq9Tp48ab1n+fLlyuFwqM2bN6s9e/ao2267jUtFldKujlOK46TUmcvXQ0ND1bJly9QXX3yhnn32WRUREaGeeeYZ6z0cJ6WmTp2qevXqZV2ivXnzZnXBBReoBQsWWO/hOLVeUIaQUko98cQTKjExUXXp0kX95je/sS5H7ohE5Jyv9evXW+9pbGxUDz30kIqLi1N2u11dffXVas+ePeYGHSQ8Q4jjdMZrr72mBg0apOx2u+rfv79au3at1s9xUsrlcql58+apPn36qLCwMNWvXz+Vk5Oj6urqrPdwnFqP5wkBAIwJujUhAEDHQQgBAIwhhAAAxhBCAABjCCEAgDGEEADAGEIIAGAMIQQAMIYQAgAYQwgBAIwhhAAAxvw/PDdAl+nDWU4AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img_list[4][:,:,0], cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801dcced-9ca1-4e44-a0da-8215cbc4e9a0",
   "metadata": {},
   "source": [
    "That looks alright! It's important that the size of the symbols is roughtly similar to the symbol size out of the pre-processing pipeline.\n",
    "\n",
    "Now we can set up the neural network. For optimal performance, I'll use the EfficientNetB0 network, and re-train it using my data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8c966a2-2f01-4348-ba55-eee6f746d504",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df7b8990-e181-425f-9e7a-316f186be278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " efficientnetb0 (Functional)  (None, 1280)             4049571   \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 1280)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                81984     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 100)               6500      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,142,215\n",
      "Trainable params: 4,100,192\n",
      "Non-trainable params: 42,023\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "efficient_net = EfficientNetB0(\n",
    "    weights='imagenet',\n",
    "    input_shape=(100,100, 3),\n",
    "    include_top=False,\n",
    "    pooling='max'\n",
    ")\n",
    "\n",
    "model = Sequential()\n",
    "model.add(efficient_net)\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(units = 64, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(units = 64, activation='relu'))\n",
    "model.add(Dense(units = 100, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate = 0.0005),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df9ff22-c268-41bb-b185-7982472d0b3f",
   "metadata": {},
   "source": [
    "Now we can fit the model. NB: on my local machine this took about 4hrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6d7cb9-feb3-4ce1-a8e1-b24c41dfcbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "378/378 [==============================] - 1274s 3s/step - loss: 1.0933 - accuracy: 0.7599 - val_loss: 0.3551 - val_accuracy: 0.9149\n",
      "Epoch 2/30\n",
      "378/378 [==============================] - 1197s 3s/step - loss: 0.3364 - accuracy: 0.9117 - val_loss: 0.2639 - val_accuracy: 0.9318\n",
      "Epoch 3/30\n",
      "378/378 [==============================] - 1265s 3s/step - loss: 0.2488 - accuracy: 0.9314 - val_loss: 0.2171 - val_accuracy: 0.9430\n",
      "Epoch 4/30\n",
      "378/378 [==============================] - 1210s 3s/step - loss: 0.2132 - accuracy: 0.9395 - val_loss: 0.2079 - val_accuracy: 0.9435\n",
      "Epoch 5/30\n",
      "378/378 [==============================] - 1178s 3s/step - loss: 0.1896 - accuracy: 0.9449 - val_loss: 0.1905 - val_accuracy: 0.9502\n",
      "Epoch 6/30\n",
      " 44/378 [==>...........................] - ETA: 17:10 - loss: 0.1583 - accuracy: 0.9539"
     ]
    }
   ],
   "source": [
    "h = model.fit(train_ds, validation_data=val_ds, epochs=30, callbacks=[es])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7e21b3-cf80-4ee0-b185-276a03af8753",
   "metadata": {},
   "source": [
    "An accuracy of 95% on the validation data is pretty good! Now we can save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f0ad0245-fde9-42c8-ab86-881229cd005e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '../CNN_model'\n",
    "if os.path.isdir(model_dir) == False: os.mkdir(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5e74aed0-ae5d-4398-b5ad-a0ab0c3aa6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_dir + '/efficientnet_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "897a65fd-8c45-4f6b-8952-aa81deeb2c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " efficientnetb0 (Functional)  (None, 1280)             4049571   \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 1280)              0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 64)                81984     \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 100)               6500      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,142,215\n",
      "Trainable params: 4,100,192\n",
      "Non-trainable params: 42,023\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3820d2d-118f-4e7e-bdc9-70c8d561a765",
   "metadata": {},
   "source": [
    "Let's also try efficientnet B2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "edbffaa0-af5b-4b41-8e28-034543e59d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67cb99b1-e7c3-4c73-b006-07d9f8b031e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb2_notop.h5\n",
      "31790344/31790344 [==============================] - 2s 0us/step\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " efficientnetb2 (Functional)  (None, 1408)             7768569   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1408)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                45088     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 100)               3300      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,816,957\n",
      "Trainable params: 7,749,382\n",
      "Non-trainable params: 67,575\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "efficient_net = EfficientNetB2(\n",
    "    weights='imagenet',\n",
    "    input_shape=(100, 100, 3),\n",
    "    include_top=False,\n",
    "    pooling='max'\n",
    ")\n",
    "\n",
    "model = Sequential()\n",
    "model.add(efficient_net)\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(units = 32, activation='relu'))\n",
    "model.add(Dense(units = 100, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate = 0.0005),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "477e722f-2cab-420f-8369-244238642e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "396/396 [==============================] - 1934s 5s/step - loss: 0.7638 - accuracy: 0.8303 - val_loss: 0.3419 - val_accuracy: 0.8851\n",
      "Epoch 2/30\n",
      "396/396 [==============================] - 1889s 5s/step - loss: 0.2237 - accuracy: 0.9363 - val_loss: 0.2042 - val_accuracy: 0.9452\n",
      "Epoch 3/30\n",
      "396/396 [==============================] - 1886s 5s/step - loss: 0.1735 - accuracy: 0.9474 - val_loss: 0.1916 - val_accuracy: 0.9462\n",
      "Epoch 4/30\n",
      "396/396 [==============================] - 1892s 5s/step - loss: 0.1474 - accuracy: 0.9533 - val_loss: 0.2009 - val_accuracy: 0.9450\n",
      "Epoch 5/30\n",
      "396/396 [==============================] - 1902s 5s/step - loss: 0.1356 - accuracy: 0.9567 - val_loss: 0.1943 - val_accuracy: 0.9491\n",
      "Epoch 6/30\n",
      "396/396 [==============================] - 1894s 5s/step - loss: 0.1204 - accuracy: 0.9615 - val_loss: 0.1752 - val_accuracy: 0.9546\n",
      "Epoch 7/30\n",
      "396/396 [==============================] - 1890s 5s/step - loss: 0.1127 - accuracy: 0.9632 - val_loss: 0.1933 - val_accuracy: 0.9497\n",
      "Epoch 8/30\n",
      "396/396 [==============================] - 1883s 5s/step - loss: 0.1002 - accuracy: 0.9668 - val_loss: 0.2040 - val_accuracy: 0.9478\n",
      "Epoch 9/30\n",
      "396/396 [==============================] - 1886s 5s/step - loss: 0.0939 - accuracy: 0.9694 - val_loss: 0.2072 - val_accuracy: 0.9482\n",
      "Epoch 10/30\n",
      "396/396 [==============================] - 1888s 5s/step - loss: 0.0861 - accuracy: 0.9713 - val_loss: 0.1770 - val_accuracy: 0.9545\n",
      "Epoch 11/30\n",
      "396/396 [==============================] - 1886s 5s/step - loss: 0.0828 - accuracy: 0.9729 - val_loss: 0.2083 - val_accuracy: 0.9498\n"
     ]
    }
   ],
   "source": [
    "h = model.fit(train_ds, validation_data=val_ds, epochs=30, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220db09b-7c81-4179-9c81-080ed56ace5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
